<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport" />
    <meta content="#222222" name="theme-color" />
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet" />
    <link href="assets/stylesheet.css" rel="stylesheet" />
    <title>Narutatsu Ri</title>
</head>

<body>
    <div id="navbar-placeholder"></div>
    <div class="container pt-5">
        <div class="row justify-content-center">
            <div class="col-lg-8">
                <p class="aligned-text">
                    Below are a few papers that I scribbled some notes for on a whim; this is mainly here for me to keep
                    myself accountable for reading papers :).
                    <br />
                    Some of these are incomplete, and others may be missing certain parts.
                    Although I mainly created it to help myself quickly locate some papers I've read in the past
                    using <code>Ctrl+F</code>, please feel free to do the same if you happen to find it useful.
                </p>
                <div class="paper-titles">Neural Computation Models</div>
                <details>
                    <summary>On a model of associative memory with huge storage capacity (Demircigil et al., 2017)
                    </summary>
                    <div class="content">

                        <p>
                            The paper addresses associative memory models and how many patterns can be reliably stored
                            and retrieved in a generalized Hopfield-like network.
                        </p>
                        <p style="color: gray;">
                            <strong>Hopfield Network ("Neurons that fire together wire together").</strong>
                            An \(N\)-node fully-connected network where each node (neuron) can take values in \(\{-1,
                            +1\}\). Given \(M\) patterns (each one an \(N\)-length bit-string denoted by
                            \(\{\xi^\mu\}_{\mu=1}^M\)) we want to "memorize"/"program" these patterns into the network.
                            The interaction (synaptic) strength between nodes \(i\) and \(j\) is defined as
                            \[
                            J_{ij} \;=\; \sum_{\mu=1}^M \; \xi_i^\mu \, \xi_j^\mu,
                            \]
                            where \(\xi_i^\mu\) is the \(i\)-th component of the \(\mu\)-th pattern.
                            <br />
                            The Hopfield energy of a configuration \(\sigma = (\sigma_1, \ldots, \sigma_N)\) is
                            \[
                            H(\sigma) = - \sum_{i&lt; j} J_{ij}\sigma_i \sigma_j,
                            \]
                            where \(J_{ij} = J_{ji}, J_{ii} = 0\).
                            <br />
                            Taking the derivative of \(H(\sigma)\), we run the update rule (synchronously or
                            asynchronously) for neuron \(i\) in configuration \(\sigma=(\sigma_1,\ldots,\sigma_N)\) as
                            \[
                            T_i(\sigma)
                            \;=\;
                            \mathrm{sign}\!\Bigl(\sum_{j=1}^N J_{ij}\,\sigma_j\Bigr),
                            \]
                            which becomes the value of node \(i\) at the next time step.
                            This iterative process continues until the network converges (i.e., reaches a stable state).
                            The Hopfield energy decreases or stays the same whenever a neuron updates its state
                            according to this rule (i.e., \(H(\sigma)\) acts as a Lyapunov function) and the network
                            converges to a local minimum.
                        </p>
                        <p>
                            Stable configurations of the Hopfield dynamics correspond to local minima of \(H(\sigma)\),
                            and hence the patterns that the network is designed to store naturally appear as local
                            minima: when we start the network close to one of these minima (i.e., configurations within
                            a certain "radius of attraction" around a stored pattern), it will tend to settle into said
                            closest pattern after repeated updates.
                        </p>
                        <p>
                            <strong>Under Exact Stability vs. Allowing Errors.</strong>
                            When no errors are desired (i.e., each stored pattern is exactly stable), we cannot do
                            better than \(M\sim N/\log N\) (specifically, we can store \(M\approx C\,N/\log N\) patterns
                            such that a fixed pattern is stable w.h.p. if \(C &lt; 1/2\)).
                            If we allow a small fraction of errors in the retrieved state, we can push \(M\) up to
                            \(\alpha N\) for some \(\alpha &lt; 0.138\).
                            Thus, exact stability of all stored patterns imposes more stringent capacity constraints,
                            whereas permitting a fraction of spins to be incorrect lets us store more patterns without
                            destabilizing retrieval.
                        </p>
                        <p>
                            <strong>Pushing Beyond the \(N / \log N\) Bound.</strong>

                            This paper attempts to push beyond the \(N / \log N\) bound for when we want every pattern
                            to be exactly stable by modifying the network's interaction function. Concretely, the
                            generalized Hopfield energy form can be written as (cf. Krotov-Hopfield Generalized Model):
                            \[
                            T_{i}(\sigma) = \mathrm{sign}\!\Bigl(\sum_{\mu=1}^M
                            \bigl[ F\bigl(\xi^\mu_i + \sum_{j\neq i} \xi^\mu_j \sigma_j\bigr)
                            \;-\;
                            F\bigl((-1) \cdot \xi^\mu_i + \sum_{j\neq i} \xi^\mu_j \sigma_j\bigr)
                            \bigr]\Bigr),
                            \]
                            where setting \(F(x)=x^2\) reduces to the classical Hopfield model.
                            <br />
                            The motivation is that the classical energy function changes too slowly near stored
                            patterns, which limits storage capacity (intuition: the "pull" toward a stored pattern is
                            relatively weak in the classical Hopfield energy, so the network cannot reliably correct
                            errors when too many patterns are stored). A higher-order or exponential function can cause
                            the network to more sharply lock onto a correct pattern, thereby improving capacity while
                            preserving a non-negligible basin of attraction for each stored pattern.
                        </p>
                        <p>
                            Hence, the paper first considers an alternative \(F(x) = x^n\), which can be shown to push
                            the capacity from \(\sim N\) to \(\sim N^{n-1}\).
                            This result can also be framed in the so-called \(n\)-spin generalization (i.e., products of
                            \(n\) different spins at once in the network's energy).
                            They show that as \(n\) grows, one can store superlinear numbers of patterns (e.g., up to
                            \(N^{n-1}\), up to constants and logarithmic factors), yet each pattern remains associative.
                        </p>
                        <p>
                            As a natural extension to polynomials, the paper focuses on the case \(F(x) = e^x\) (the
                            exponential function can be written as a formal power series \(e^x = \sum_{k=0}^\infty
                            x^k/k!\)). Using a large-deviation/Cramer argument, they show that we can store \(M =
                            \exp(\alpha N) + 1\) patterns for \(\alpha &lt; \log(2)/2\), when allowing a fraction \(\rho
                            &lt; 1/2\) of corrupted spins. More specifically, if the network starts from a configuration
                            that differs from a stored pattern in \(\rho N\) spins, it still recovers that pattern
                            w.h.p. when \(\alpha &lt; \frac{ I(1-2\rho)}{2}\), where \(I(x) =
                            \tfrac{1}{2}\bigl((1+x)\log(1+x) + (1-x)\log(1-x)\bigr)\).
                            <br />
                            In words, exponential interactions allow storing exponentially many patterns, and each
                            pattern can correct a finite fraction of errors, and no "associativity collapse" occurs.
                        </p>
                        <p>
                            In essence, by tuning \(F(x)\) to be increasingly higher order all the way to \(e^x\), the
                            "signal" from the correct pattern outcompetes the "noise" due to the massive number of other
                            patterns on the order of \(\exp(\alpha N)\), provided \(\alpha\) is below the threshold
                            given by \(I(1-2\rho)/2\). The large-deviation bounds ensure that the overwhelming majority
                            of random crosstalk events do not derail retrieval. Thus, the exponential model pushes the
                            memory capacity to an exponential scale while still guaranteeing an attractive fixed point
                            for each stored pattern.
                        </p>
                    </div>
                </details>
                <details>
                    <summary>The Impact of Positional Encoding on Length Generalization in Transformers (Kazemnejad et
                        al., 2023)</summary>
                    <div class="content">

                        <p>
                            It is well-known that transformers often fail to extrapolate to longer sequence lengths than
                            those they were trained on (i.e., length generalization), and positional encodings (PE)
                            appear to be crucial in this limitation. This paper examines the role of positional encoding
                            (APE, T5’s Rel, ALiBi, Rotary) in this problem in decoder-only Transformers.
                            <br />
                            Using synthetic tasks (Copy, Reverse, Addition, Summation, Polynomial Evaluation, Sorting,
                            Parity, LEGO, SCAN, PCFG), they train on examples up to length \(L\) and test on examples up
                            until length \(2L\) while evaluating with exact-match accuracy.
                            <br />
                            They show that:
                        <ul>
                            <li>
                                All tested PEs and NoPE (no positional encoding) perform near-perfect in-distribution,
                                but commonly used PEs often fail at length generalization (with T5's Relative Bias
                                performing best on unseen longer lengths. More specifically, T5 \(&gt;\) ALiBi \(&gt;\)
                                APE, Rotary).
                            </li>
                            <li>
                                Using a constructive argument, they show that a single attention head (i.e., the first
                                layer) can count the position in an absolute positional manner.
                                Using these absolute positions and once they are in the hidden state, a relative
                                position bias can also be emulated. As measured via distributional dissimilarity
                                (Jensen-Shannon divergence), the manner in which NoPE has chosen to distribute attention
                                strongly resembles T5's Relative PE scheme.
                            </li>
                            <li>
                                No positional encoding ften outperforms all PEs.
                            </li>
                            <li>
                                Chain-of-thought only sometimes helps with length extrapolation, but do not make the
                                choice of PE irrelevant.
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <div class="paper-titles">Safety/Alignment</div>
                <details>
                    <summary>Anwar et al. (2024) (Section 3.4 Only)</summary>
                    <div class="content">
                        <p><strong>Title:</strong> Foundational Challenges in Assuring Alignment and Safety of Large
                            Language Models</p>
                        <p>
                            Existing methods for interpreting model behavior lack faithfulness.
                            This section touches on interpretability-based methods (think mechanistic interpretability)
                            and explainability-based methods (generating natural language explanations).
                            <br />
                            Critical challenges include:
                        <ul>
                            <li><i>Interp. methods assume a priori that internal model reasoning works in specific ways,
                                    which leads to questionable assumptions.</i>
                                <br />
                                For example, Integrated Gradients, Shapley values assume non-linear behavior can be
                                sufficiently
                                explained with linear models, which turned out to have arbitrarily worse failure cases;
                                interpretabiity of neurons → may be polysemantic; linear probing easily learns spurious
                                features instead of actual features.
                            <li><i>Neural networks do not have incentive to be inherently interpretable/explainable.</i>
                                <br />
                                Particularly a concern for domains where AI outperforms humans, because explanations
                                that
                                are understandable by humans may not exist/be possible. <b>Representation alignment</b>
                                helps AI
                                models learn human-aligned representations, but works in a human-in-the-loop manner so
                                hard to scale up
                            </li>
                            <li><i>Scalable evaluation of the faithfulness of a generated explanation is hard.</i>
                                <br />
                                Measuring faithfulness requires gaining access to the true underlying internal
                                reasoning, but this is inaccessible (and why we want explainability to begin with).
                                An important direction is that <b>there is a need for benchmarks to standardize metrics
                                    of success and to help create better standards for evaluating explanation
                                    faithfulness.</b> <b>In particular, for detecting alignment failures, metrics that
                                    focus on
                                    worst-case is necesary.</b>
                            </li>
                            <li><i>Desirably identified patterns should lead to modifiable behavior, but unclear how
                                    this can be done.</i></li>
                            </li>
                        </ul>
                        </p>
                        <p>
                            Challenges for explainability, i.e., externalized reasoning primarily lies in
                            faithfulness and steerability: Attempts to make reasoning visible in a natural language
                            manner (e.g., chain-of-thought-style methods) can be misleading/unfaithful and
                            insensitive to "edits" made to the reasoning.
                            Open questions include:
                        <ul>
                            <li>
                                Understanding to what extent externalized reasoning is causally responsible for improved
                                performance on various reasoning tasks.
                            </li>
                            <li>
                                Understanding the extent to which the training directly or implicitly incentivizes
                                unfaithfulness.
                            </li>
                        </ul>
                        </p>

                    </div>
                </details>
                <div class="paper-titles">Analysis of Attention-based Models</div>
                <details>
                    <summary>Do Transformers Parse while Predicting the Masked Word? (Zhao et al., 2023)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> PCFG, masked language modeling, inside-outside algorithm, parse
                            tree</p>
                        <p>
                            This paper theoretically analyzes the empirical observation that contextual embeddings
                            obtained from attention-based models (e.g., BERT) trained with masked language modeling
                            objective somewhat encode parse tree information. Previous empirical studies show that this
                            encoding can use semantic cues in natural language to falsely indicate that the embeddings
                            contain syntax parsing information. This paper removes semantics from the equation by using
                            strings generated with PCFGs as data.
                        <ul>
                            <li>There exists a transformer architecture (for both hard and soft attention) that
                                implements the Inside-Outside algorithm to recover parse trees for PCFGs. But, the
                                number of layers and dimensions required is unrealistic.</li>
                            <li>They consider \textit{approximating} the Inside-Outside algorithm with transformers,
                                which yields a more realistic-size transformer architecture without losing too much
                                performance.</li>
                            <li>For a PCFG, the Inside-Outside algorithm yields the optimal solution for the masked
                                language modeling objective.</li>
                            <li>Experimentally, they use a linear probe and show:
                                <ul>
                                    <li>the contextual embeddings capture the structure of the constituency parse trees.
                                    </li>
                                    <li>the intermediate-layer representations of the transformer can be used to predict
                                        marginal probabilities computed in the Inside-Outside algorithm.</li>
                                </ul>
                            </li>
                        </ul>
                        Punchline: To implement inside-outside algorithm perfectly w/ transformer architecture, a lot of
                        layers is needed. But transformers can approximate inside-outside algorithm with realistic
                        number of layers.
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Pay Attention to MLPs (Liu et al., 2021)</summary>
                    <div class="content">

                        <p>

                            This paper studies the necessity of self-attention layers in transformers by proposing a new
                            architecture called gMLP, which incorporates cross-token interactions but does not utilize
                            self-attention layers. Given input \(X \in \mathbb{R}^{N\times d}\), one vanilla gMLP layer
                            is written as:

                            \[
                            \Big(\sigma(XU) \odot (W \sigma(XU) + b) \Big) V + X,
                            \]
                            where \(X\in \mathbb{R}^{n\times d}\) is the input to the layer, \(\sigma(\cdot)\) denotes
                            an activation function, and \(U, V, W\in \mathbb{R}^{n\times n}\) are learnable parameters
                            (no special constraints). They refer to the element-wise product
                            \[
                            s(\cdot) = \sigma(XU) \odot f_{W, b}(X) = \sigma(XU) \odot (W \sigma(XU) + b)
                            \]
                            as the "spatial gating unit," hence the "g" in "gMLP." The "mixing" part is handled by
                            the \(W\) matrix and \(W, b\) need to be initialized to near-zero and all-ones matrix for
                            empirical training stability.

                            It is also shown that when one additional self-attention module is included in gMLP in the
                            form:
                            \[
                            s(\cdot) = \sigma(XU) \odot \big( (W \sigma(XU) + b) + A(X) \big)
                            \]
                            where \(A(X)\) denotes one self-attention head, then better performance than both
                            transformers and gMLP is achievable.
                        <ul>
                            <li>gMLP performs better on vision tasks than transformers with same number of parameters
                                and training time.</li>
                            <li>gMLP trained on masked language modeling objective has better pre-training perplexity
                                than BERT, and does comparably to BERT on NLP tasks that do not require cross-lingual
                                alignment.</li>
                            <li>gMLP requires a scalar multiple (~3) of the number of parameters to match BERT's
                                performance on natural language tasks that require pairwise attention.</li>
                        </ul>
                        Punchline: This architecture is arguably simpler than self-attention and achieves comparable
                        performance on vision tasks, but not on natural language tasks that require token mixing.
                        <br />
                        -&gt; Perhaps self-attention is comparably "cost-efficient" in terms of parameters?
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Limits to Depth-Efficiencies of Self-Attention (Levine et al., 2020)</summary>
                    <div class="content">

                        <p>
                        <ul>
                            <li>When the depth is less than a polynomial factor of the width, then increasing depth
                                improves separability.</li>
                            <li>When the depth is more than a polynomail factor of the width, then increasing depth has
                                minimal benefit. A polynomially wide network can surpass an arbitrarily deep network.
                            </li>
                            <li>Empirically, for sufficiently small networks, deep networks perform worse than shallow
                                networks. i.e. for a fixed width, a network can be too shallow (where performance can
                                increase by adding more layers) or can be too deep (where performance degrades).</li>
                        </ul>
                        Punchline: Depth is not all you need for self-attention networks, in contrast to other neural
                        architectures.
                        </p>
                    </div>
                </details>
                <div class="paper-titles">Cross-Lingual Transfer</div>
                <details>
                    <summary>Normalized Word Embedding and Orthogonal Transform for Bilingual Word
                        Translation (Xing et al., 2015)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> Cross-lingual alignment, linear transformation, orthogonal
                            transformation</p>
                        <p> Uses static word embeddings (word2vec-style). Claims Mikolov et al.
                            (2013)'s approach is ill-posed because the same similarity metrics are not used during
                            training and testing. Proposes to length-normalize vectors during training.</p>
                        <ul>
                            <li>
                                First paper to propose the orthogonal transform approach within
                                projection-based methods for cross-lingual alignment.
                            </li>
                            <li>
                                Approach for learning linear projection between embeddings of different dimensions is
                                ad-hoc.
                            </li>
                            <li>
                                Linear projection is learned using data from Google Translate.
                            </li>
                        </ul>

                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                Wonder if unsupervised learning of the projection is possible.
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Learning principled bilingual mappings of word embeddings while
                        preserving monolingual invariance (Artetxe et al., 2016)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual alignment, linear transformation, orthogonal
                            transformation</p>
                        <p> Direct follow-up to Xing et al. (2015). Combines the optimization
                            objectives of Xing et al. (2015) and Faruqui and Dyer (2014). Evaluates word embeddings and
                            the learned transformation on translational similarity (How close the mapped vector from
                            language \(A\) is to the word in language \(B\) with "same" meaning.) and word embeddings
                            only
                            on analogy solving task.
                        <ul>
                            <li>
                                Length-normalization is not important over orthogonal matrix.
                            </li>
                            <li>
                                Adds mean centering constraint to optimization and states the optimization is equivalent
                                to Faruqui and Dyer (2014) but does not alter monolingual embeddings manually.
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Offline bilingual word vectors, orthogonal transformations and the
                        inverted softmax (Smith et al., 2017)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual alignment, orthogonal transformation, hubness
                            problem</p>
                        <p>
                            Follow-up work to all orthogonal transformation literature. Shows exisiting methods can be
                            unified into an optimization procedure using SVD. Proves orthogonal transformations are
                            self-consistent (Similarity between words in language A and B should be same as the other
                            way around, i.e. \(S_{i,j} = S_{j, i}\)). Introduces an "inverted softmax" approach to
                            choosing the translated word after a linear map is learned that combats the hubness problem
                            by ensuring hubs (Words that are the nearest neighbor for a lot of words) are not chosen as
                            the translation word with high probability. Also shows a "pseudo-dictionary" (A dictionary
                            between two languages that are created by counting the number of shared characters between
                            two words) is sufficient for achieving comparable (better) performance to supervised (with
                            dictionary) cases.
                            Moreover, instead of learning word-word transformations, a learned sentence-sentence
                            transformation on the word embeddings can fare well on both sentence and word translation.

                        <ul>
                            <li>
                                A pseudo-dictionary is enough to achieve good performance (42% for 1-NN, 59% for 5-NN).
                            </li>
                            <li>
                                Orthogonal transformations learned between word-word and sentence-sentence fare the same
                                performance on word-word translation.
                            </li>
                            <li>
                                When the sentence-sentence transformations are applied to word-word translation, using
                                the inverted softmax greatly boosts performance.
                            </li>
                        </ul>
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                What's the main difference between this paper and Artetxe et al. (2016)? Seems like the
                                "unifying" part is very similar.
                            </li>
                            <li>
                                Are the linear transformations learned in the word-word case and sentence-sentence case
                                very similar?
                            </li>
                            <li>
                                Surely, pseudo-dictionaries are a very bad source to obtain word correspondences. Is the
                                good performance because of the orthogonal transformation or inverted softmax?
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Learning bilingual word embeddings with (almost) no bilingual data (Artetxe et al., 2017)
                    </summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual alignment, bilingual lexicon induction</p>
                        <p> Using previous methods that learn a linear transformation,
                            considers a bootstrapping method using a very small dictionary and iteratively improving it
                            by retraining and regenerating a word-word dictionary by choosing nearest neighbor of
                            transformed vectors. The learned transformation tends to be the same regardless of starting
                            dictionary, and observed errors are very similar to that of Artetxe et al. (2016).
                        <ul>
                            <li>
                                Bootstrapping from a very small (~25 word pairs) dictionary performs as well as
                                using a large dictionary.
                            </li>
                            <li>
                                Using very small starting dictionary leads to similar final dictionary as Artetxe et al.
                                (2016) indicates that learning a transformation in an unsupervised manner could be
                                possible.
                            </li>
                            <li>
                                However, (while not mentoned in the paper) performance does correlate with the
                                similarity between the two languages.
                            </li>
                        </ul>
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                This likely does not work on two very different languages, because the text embeddings
                                learned in the languages likely take a very different structure.
                            </li>
                            <li>
                                At each time step, are we using all words as anchors or only the most "confident" words?
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Word translation without parallel data (Conneau et al., 2018)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual alignment, unsupervised, hubness problem</p>
                        <p>
                            Introduces cross-lingual adversarial alignment method that performs on par/better than
                            previous data-dependent methods in an unsupervised setting and a scoring metric to evaluate
                            goodness of transformation during training. Also mitigates hubness problem by considering
                            the same idea as in Artetxe et al. (2017) and introducing CSLS (Intuition: word similarity
                            is discounted by average word similarity with other words of both words. A hub will take a
                            large average word similarity value).
                        <ul>
                            <li>
                                First paper to introduce a fully unsupervised way to learn orthogonal transformations
                                between two language embeddings.
                            </li>
                            <li>
                                Works reasonably well for relatively different languages too (English-Esperanto).
                            </li>
                        </ul>
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                Does this not assume the embedding space structure of the two languages are very
                                similar? English and Esperanto are arguably relatively similar.
                            </li>
                            <li>
                                Will this still work when the vocabulary size of the two languages are drastically
                                different?
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Improving Cross-Lingual Word Embeddings by Meeting in the Middle (Doval et al., 2018)
                    </summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual transfer</p>
                        <p> Based on previous work in meta-learning where averaging embeddings
                            learned under different settings result in better-performing embeddings, this paper
                            considers learning another linear transformation after an orthogonal transformation is
                            learned, where this additional transformation looks to map the embeddings words that
                            correspond to each other to the average between the two embeddings. Learns another linear
                            mapping to map embeddings to the average embedding between the two langauge embeddings.
                            Applying this mapping to the original embeddings does not work.
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Loss in Translation: Learning Bilingual Word Mapping with a Retrieval
                        Criterion (Joulin et al., 2018)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual transfer, hubness problem</p>
                        <p> Argues solutions to the hubness problem are unsatisfactory "because
                            the loss used for inference is not consistent with that employed for training." Incorporates
                            CSLS into the training objective when learning the transformation. Argues removing
                            orthogonality constraint does not degrade word analogy and word similarity performance when
                            transforming Spanish embeddings to English. Essentially, incorporated CSLS variant into
                            training.</p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                I didn't see why the unsatisfactory part of previous work is unsatisfactory.
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Learning Multilingual Word Embeddings in Latent Metric Space: A
                        Geometric Approach (Jawanpuria et al., 2018)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual alignment, linear transformation</p>
                        <p> Follow-up work to previous cross-lingual alignment works that
                            employ linear transformation, but multiple language correspondings are learned
                            simultaneously by mapping them into a "latent space." Considers individual orthogonal
                            transformations for each language, then a universal positive definite matrix applied
                            afterwards. Individual transformations and universal matrix are jointly trained. Also
                            considers the low-resource setting where dictionary data is scarce and shows bootstrapping
                            to iteratively improve dictionary achieves on par performance as previous work (Artetxe et
                            al. (2017)).
                        <ul>
                            <li>
                                Can learn transformations between multiple languages simultaneously.
                            </li>
                            <li>
                                Also functions on par with existing approaches in low-resource settings.
                            </li>
                        </ul>
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                Unclear if orthogonal transformations are trained all at once, or each langauge pair is
                                trained individually.
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Cross-Lingual Alignment of Contextual Word Embeddings, with
                        Applications to Zero-shot Dependency Parsing (Schuster et al., 2019)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual transfer, linear transformation, contextualized
                            embeddings, parsing</p>
                        <p> Considers contextualized word embeddings (from ELMo) and uses
                            anchors (the average of all contextualized embeddings of the same word) as representation
                            for each word. Considers three settings: 1) when dictionary is available, apply standard
                            linear transformation learning methods to anchors, 2) Apply unsupervised learning (as in
                            Artetxe et al. (2017), Conneau (2018), etc.) to anchors, and 3) apply standard linear
                            transformation learning methods to contextualized embeddings.
                            Also considers low-resource target language case, in which they consider adding a
                            regularization term on the Euclidean distance between embeddings from the source and target
                            dictionary.
                        <ul>
                            <li>
                                First paper to use contextualized word embeddings.
                            </li>
                            <li>
                                Model can perform well zero-shot, i.e., train on different language than testing
                                language.
                            </li>
                            <li>
                                Uses anchors to capture one representation for each word.
                            </li>
                        </ul>
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>Surely we can do more than just apply existing methods for static word embeddings?</li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>How Multilingual is Multilingual BERT? (Pires et al., 2019)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual transfer, contextualized embeddings</p>
                        <p>mBERT performs well on zero-shot POS tagging across languages that
                            have similar typographical properties (even when the tokens are not the same) but poorly on
                            different languages. When considering an alternate representation of a language (e.g., Hindi
                            written in Latin), the model fails. When a language trained in one script (e.g., Hindi) is
                            tested on an alternate script (i.e., Devanagari), the model performs comparably well as the
                            original language. Model performance on translation is seen to be maximized when the
                            contextualized representations of the middle layers of the model are used (authors attribute
                            diminsihing performance of later layers to no fine-tuning).
                        <ul>
                            <li>
                                mBERT--BERT trained on masked language modeling objective for 104 languages--can
                                generalize a fine-tuned task (e.g., NER) to a different language.
                            </li>
                            <li>
                                Transferability diminishes as language typographic similarity decreases, i.e.,
                                syntactic structure is the main source of information necessary for mBERT to perform
                                zero-shot inference.
                            </li>
                        </ul>
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                Perhaps OOD generalization for syntactically similar languages indicate the model is
                                learning some form of a generalized parser?
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing (Wang et al., 2019)
                    </summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual alignment, linear transformation</p>
                        <p> Considers learning a linear transformation on contextualized
                            embeddings from mBERT and denotes it CLBT. Learn a mapping that maps embeddings from source
                            language to target language, and plugs transformed embeddings into neural parsers.
                            Performance is better than mBERT and XLM.
                            Learned linear mapping between contextualized embeddings of two different languages.
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                I don't see any novelty in the paper beyond what Schuster et al. (2019) already
                                showed.
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of
                        BERT (Wu and Dredze, 2019)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual transfer</p>
                        <p> A case study of mBERT on five NLP tasks, extending Pires (2019).
                            Shows that mBERT's representations perform on par with and sometimes better than
                            cross-lingual embeddings. Moreover, shows how mBERT's performances vary depending on which
                            layer's representation is used; shows using later layers' embeddings lead to worse
                            performance. Shows representations of all layers perform well on natural language
                            identification. Shows cross-lingual transfer performance correlates with the number of
                            subtoken overlaps between languages.
                        <ul>
                            <li>
                                mBERT performs on par with cross-lingual alignment methods.
                            </li>
                            <li>
                                Cross-lingual transfer performance depends on subtoken alignment rate.
                            </li>
                            <li>
                                mBERT is good at identifying languages (i.e., representations of different
                                languages are clearly mapped to different sections in the representation space)
                            </li>
                        </ul>
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                Results slightly differ from Pires (2019); they show cross-lingual transfer performance
                                is good even with completely different tokens. I suppose this study didn't take into
                                account the typography of the language when studying subtoken overlap?
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Are Girls Neko or Shōjo? Cross-Lingual Alignment of Non-Isomorphic
                        Embeddings with Iterative Normalization (Zhang et al., 2019)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual alignment</p>
                        <p> Considers method to preprocess static word embeddings to ensure
                            applying an orthogonal transformation between language embeddings with different structures
                            is suitable. The method is simply iteratively normalizing the word vectors then centering
                            them until convergence. Performance boosts on word translation accuracy is most significant
                            in distant languages (Japense, Chinese) whereas increase in similar, high-resource languages
                            (Spanish) are minimal.
                            Iterative normalization strongly boosts word translation performance for distant languages
                            than English.
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                What about other tasks besides word translation?
                            </li>
                            <li>
                                Does the same idea hold for contextualized embeddings?
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Multilingual Alignment of Contextual Word Representations (Cao et al., 2020)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> multilingual alignment</p>
                        <p>
                            Considers fine-tuning mBERT using a large cross-lingual parallel corpus such that the
                            contextualized representations of words with the same meaning are pulled closer (with some
                            regularization term).
                            Introduces contextual word alignment task, evaluates all models on it and NLI.
                            Fine-tuning mBERT representations improves cross-lingual alignment too.</p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                What about performance on other tasks (XNLI, etc.)?
                            </li>
                            <li>
                                Not quite convinced this new proposed task is a worthy replacement benchmark of existing
                                ones.
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Do Explicit Alignments Robustly Improve Multilingual Encoders? (Wu and Dredze, 2020)
                    </summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> cross-lingual alignment, fine-tuning, mBERT, XLM-R</p>
                        <p> Considers a follow-up work to fine-tuning multilingual LLMs where a
                            contrastive learning objective (weak and strong) are considered. More extensive experiments
                            (in the sense that multiple seeds are used on the same model) are conducted to show existing
                            methods (linear transformation, \(L_2\) alignment (Cao (2020))) do not improve performance
                            over mBERT's representations and contrastive learning marginally improves performance. mBERT
                            is also beat by XLM-R which is trained on more data by 3 points on every task and is further
                            beaten by XLM-R base, indicating that increasing the amount of data and model size is far
                            more effective than existing methods.
                        <ul>
                            <li>
                                Contrastive learning objective marginally improves performance on various tasks (XNLI,
                                NER, ...).
                            </li>
                            <li>
                                Increasing the amount of data and model size improves performance the most.
                            </li>
                        </ul>
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                Can we find a way to achieve same performance with small models (or boost performance of
                                larger models) with better data usage?
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <div class="paper-titles">Prompting Frameworks</div>
                <details>
                    <summary>Self-Consistency Improves Chain of Thought Reasoning in Language
                        Models (Wang et al., 2023)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> chain-of-thought reasoning, self-consistency</p>
                        <p> Introduces self-consistency method: conduct multiple path decoding
                            for chain-of-thought reasoning and choosing the most consistent final answer by majority
                            vote.
                            Leads to improved accuracy on arithmetic and commonsense reasoning tasks using LLMs (20B ~
                            540B). Robust to sampling (decoding) strategies and imperfect prompts. Self-consistency
                            helps when CoT hurts performance (not stated specifically when this happens).
                            Also attempted using "normalized" (softmaxed) weighted sum w.r.t. decoding probabilities,
                            which provides comparable results to majority vote. </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                Can consider using weak translators to translate original question to different language
                                and conduct "self-consistency." Weak translators should suffice because this method is
                                said to be robust to prompt quality (or perhaps this is only in English?).
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Self-Refine: Iterative Refinement with Self-Feedback (Madaan et al., 2023)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> iterative refinement, self-feedback</p>
                        <p>
                            Proposes self-refine: Get LLM output, ask itself for feedback, and get new LLM output with
                            original output and feedback as input. Evaluated on wide range of tasks.

                            Results: Adding feedback (even non-specific ones, e.g., "make the code more efficient")
                            significantly boosts performance (10~30%) across three tasks, and using more informative
                            feedback increases performance a bit more. Performance increases as more iterations of
                            self-refine are run. Comparing self-refine to generating \(k\) different outputs, humans
                            still
                            preferred self-refine outputs more than all \(k\) outputs. Self-refine does not work for
                            relatively smaller models (~13B, e.g., Vicuna-13b.)

                            When self-refine fails to improve original output, the majority cause is because of
                            incorrect feedback. Self-refine still works when feedback is partially incorrect.
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                Not clarified what the stop condition is (which is later "exposed" in Huang et al.
                                (2023)).
                            </li>
                            <li>
                                Unclear whether self-refine works in other languages (mentioned in Limitations section).
                                Perhaps the capability to self-refine only emerges given sufficient data? Do
                                multilingual models have the same property or not?
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Measuring and Narrowing the Compositionality Gap in Language Models (Press et al., 2023)
                    </summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> compositionality gap, multi-hop reasoning</p>
                        <p>
                            Defines the \emph{compositionality gap}: while LLMs can answer factual (1-hop) questions
                            correctly, they struggle on questions that convolute such questions (multi-hop); the authors
                            term this gap the compositionality gap. Builds two datasets: Compositional Celebrities (CC),
                            which convolves facts using 17 different types of templates, and Bamboogle, a smalls set of
                            hand-crafted 2-hop questions. Shows that for GPT3, 1-hop accuracy can be around 80% but
                            2-hop accuracy drops to 1.2%.

                            Proposes self-ask: Ask LLM to follow up multi-hop questions by breaking it down into 1-hop
                            questions, answer each 1-hop question individually, then combine them to answer original
                            2-hop question. Leads to significant boost in performance, even compared to
                            chain-of-thought.
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                Great paper. Potential flaw is when using self-ask, the final answer would be incorrect
                                if the 1-hop question is incorrectly answered.
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Large Language Models Can Self-Improve (Huang et al., 2022)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> self-improvement, chain-of-thought, self-consistency</p>
                        <p>Proposes \emph{Language Model Self-Improved} (LMSI): Given unlabeled
                            questions, use CoT reasoning and "self-consistency" to generate labels and fine-tune model
                            on this pseudolabeled data.
                            Tested on 540B model. Improves performance on both in-domain tasks (tasks where unlabeled
                            data is taken from) and OOD tasks (marginally). Pseudolabeled data prompts are diversified
                            by using four possible templates to prevent model from producing only one style of output.
                            Also studied augmenting data. Training on self-generated data increases performance but not
                            as well as training on existing data.
                            Also tried distilling. 8B distilled model beats 62B model and 62B distilled model beats 540B
                            model. Unclear which dataset is used for this.
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>Does training in language \(A\) boost performance in language \(B\) using LMSI?</li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Large Language Models are Better Reasoners with Self-Verification (Weng et al., 2023)
                    </summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> self-verification, reasoning</p>
                        <p>
                            Proposes self-verification:
                        <ol>
                            <li>LLM generates multiple answers (Forward Reasoning).</li>
                            <li>Masks factual information in the input question and asks the LLM to answer the masked
                                part (Backward Verification) \(k\) times. </li>
                            <li>Answer in FR with the most consistency (out of \(k\) times, how many times was the
                                correct information recovered) is chosen as answer.</li>
                        </ol>
                        Model performance is slightly increased for LLMs, does not work for smaller models.
                        Also combined this with self-consistency and other methods and shows performance can be slightly
                        increased.
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                A bit misleading: it doesn't show that LLMs can self-verify, they show that they can use
                                a "trick" using LLMs to improve performance on reasoning questions.
                                Moreover, don't agree that this is "self-verification": their approach is simply Wang et
                                al. (2023) but slightly more complicated. The model itself is not verifying its own
                                answer.
                            </li>
                            <li>
                                Don't believe this method can be applied to slightly more complicated questions than
                                GSM8K. This method assumes that the questions are "simple enough" such that they are
                                correctly answerable most of the time (because their "Backward Verification" is
                                essentially doing the same thing as "Forward Reasoning").
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>LM vs. LM: Detecting Factual Errors via Cross Examination (Cohen et al., 2023)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> factual error detection, cross examination</p>
                        <p>
                            Proposes LM vs. LM as a method to automatically detect factual errors (given factual
                            statement, want to ensure examiner can accurately output whether statement is correct or
                            not):
                        <ol>
                            <li>Input statement to examiner to generate list of follow-up questions.</li>
                            <li>Ask examinee to answer all questions, which the outputs are passed to examiner.
                                (Optional) Examiner is asked whether follow-up questions are required.</li>
                            <li>Given set of examiner questions and examinee answers, examiner checks whether examinee's
                                answers are consistent. </li>
                        </ol>
                        Testing: Used closed-book open-ended datasets. Also created set of "false" claims to test how
                        well LLMs can detect false claims. Results indicate LM vs. LM performs better than existing
                        methods by 5~10%.
                        LM vs. LM fails when examinee provides incorrect but consistent outputs. 9~15% of the
                        time, LLM produces incoherent output. Examinee typically outputs incorrect answers when original
                        statement is false.
                        Not exactly about the self-correcting abilities of LLMs, and more so proposing a method to
                        conduct consistency checks with LLMs.
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Large Language Models Cannot Self-Correct Reasoning Yet (Huang et al., 2023)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> self-correction, reasoning</p>
                        <p>
                            An analytical study that questions previous papers on
                            "self-correction/self-improvement/etc." and examines whether LLMs can self-correct without
                            any form of external feedback (e.g., terminating iterative prompting when model generates
                            correct output, changing prompt instructions because previous prompt didn't work (this is
                            considered external feedback as it leverages info on learning what prompts do not work after
                            testing)). Tests three-step prompting (initial generation, critique previous output,
                            generate response using initial output and critique). Shows that previously reported
                            positive results uses external feedback in some way.

                            Uses datasets from arithmetic reasoning to question answering. Shows that LLM performance
                            decreases under self-correction regime (model is more likely to change correct -&gt;
                            incorrect
                            than vice versa) under no external feedback, which they provide their intuition as "asking
                            the model to assess the model's output might skew the model towards changing its answer."
                            Self-correction is less effective for tasks where LLMs cannot easily identify errors or
                            assess its correctness in its outputs.
                            However, self-correction is useful for altering the text style and improving
                            appropriateness.</p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                Paper suggests that searching within the answer space of the LLM can be done robustly
                                with external feedback (or self-consistency (Wang et al. (2023))). Perhaps we can do
                                even better by employing self-consistency across languages?
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Self-contrast: Better Reflection through Inconsistent Solving
                        Perspectives (Zhang et al., 2024)</summary>
                    <div class="content">

                        <p><strong>Keywords:</strong> self-contrast, self-reflection</p>
                        <p>
                            Following works that doubt LLMs can self-correct, re-shows self-correction isn't effective
                            (and sometimes decreases performance) for math reasoning and translation tasks.

                            Feedback analysis: Claims that bad feedback generated by LLMs can be classified into the
                            following: when an incorrect answer isn't fixed the model tends to assert no correction is
                            needed, when a correct answer is incorrectly changed the model's feedback is inconsistent
                            when tested multiple times. (Also shows that when incorrect output is corrected, model
                            output isn't consistent, but I am confused because this result is contradictory)

                            Proposes self-contrast framework (slightly complicated):
                        <ol>
                            <li>
                                Input paraphrases of original question with diverse styles and obtain multiple outputs
                            </li>
                            <li>
                                Ask LLM to contrast difference between each answer.
                            </li>
                            <li>
                                Based on contrast result, ask model to create a checklist to verify which case is
                                correct between discrepancies.
                            </li>
                            <li>
                                Based on answers to checklist, refine original outputs for each style, and choose most
                                consistent output.
                            </li>
                        </ol>
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                Proposing another variant to do self-reflection. It's not really answering any open
                                questions. Specifically, this question does not answer "why" self-correct doesn't work,
                                but is only examining "how" the feedback is bad for failure cases of self-correction,
                                which is not the fundamental question that needs to be answered.
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <div class="paper-titles">Cross-Lingual Consistency</div>
                <details>
                    <summary>X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained
                        Language Models (Jiang et al., 2020)</summary>
                    <div class="content">

                        <p>Built X-FACTR dataset (same structure as LAMA, but across 23
                            languages).
                            Implicitly investigates cross-lingual consistency (Section 6). Proposes fine-tuning model on
                            code-switched data (e.g., creating training samples by swapping: "Obama later reflected on
                            his years" -&gt; "Obama (in some other language) later reflected on his years").</p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                Dataset is rigorously designed.
                            </li>
                            <li>
                                Similar motivation to "Multilingual LAMA: Investigating Knowledge in Multilingual
                                Pretrained Language Models,” which is mentioned by mLAMA's author.
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Factual Consistency of Multilingual Pretrained Language Models (Fierro et al., 2022)
                    </summary>
                    <div class="content">

                        <p>
                            "Builds” mPaRaRel dataset by translating PaRaRel (38 types of cloze-style questions with ~9
                            paraphrased templates) into 45 languages. Tests monolingual consistency in different
                            languages with mBERT (110M) and XLM-RoBERTa (560M). Shows other languages have worse
                            consistency than English, which has already been shown to be monolingually inconsistent.
                            Consistency is measured simply by the total number of matches across all templates (i.e., if
                            the answer is the same for two queries that are paraphrases of each other, this is
                            considered to be consistent). Shows results for accuracy, consistency, and
                            consistency-accuracy (ones that are both consistent and accurate).
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>Only investigates monolingual consistency in different languages. Very different from
                                cross-lingual consistency.</li>
                        </ul>
                        </p>
                    </div>
                </details>
                <div class="paper-titles">Authorship Attribution</div>
                <details>
                    <summary>Learning Universal Authorship Representations (Rivera-Soto et al., 2021)</summary>
                    <div class="content">

                        <p> Previous methods for authorship attribution rely on statistical
                            methods. Recently, neural-based methods have been shown to outperform these traditional
                            methods, but A) neural methods are difficult to interpret (features are automatically
                            learned), and B) they require vast amounts of data.

                            This paper studies whether universal authorship features can be learned with training data
                            in the open-world setting (large amount of authors). They propose a self-attention (SBERT) +
                            constrastive-learning based model and use three datasets (Reddit, Amazon, Fanfiction) to
                            show that A) topic diversity is important so that model representations are learned to be
                            independent with the topic feature for author attribution, and B) given sufficient
                            independence w.r.t. topics, more authors lead to better generalizability.
                        <ul>
                            <li>
                                "Given sufficient independence w.r.t. topics, more authors lead to better
                                generalizability" is not surprising. This essentially corresponds to more training data
                                that is useful for extracting more generalized features.
                            </li>
                            <li>
                                By "large amount of authors," this seems to mean \(&gt;\) 100K authors. Is there not a
                                "crowding problem" that will happen if we operate in 512 dimensions (especially if we
                                are using inner products as the similarity metric)? I feel like there should be an
                                impossibility result.
                            </li>
                        </ul>
                        </p>
                        <p><strong>Thoughts:</strong> Raises concerns about scalability and potential crowding issues
                            when operating in limited-dimensional spaces with very many authors.</p>
                    </div>
                </details>
                <details>
                    <summary>Interpreting Embedding Spaces by Conceptualization (Simhi and Markovitch, 2023)</summary>
                    <div class="content">

                        <p>
                            Introduces a method to (non-linearly) transform LLM representations into a space where
                            features are interpretable. Uses concepts \(c\) in a pre-defined ontology (Wikipedia) and
                            considers their embeddings \(f(\tau(c))\) (\(\tau(c)\) is the natural language expression of
                            the concept and \(f\) is the LLM embedding of the expression) as independent bases in the
                            transformed space. Given text \(t\), considers \(f(t)\) as its representation. Shows that
                            the transformed representations are A) equivalent in terms of infomation encoded (tested by
                            learning classifiers on top of representations) and B) meaningful in that the concepts are
                            informative of the original sentence.

                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                (I'm aware that this is already done but) I'm wondering how this can be applied
                                reasonably to author attribution: the difficulty seems to be figuring out reasonable
                                concepts \(c\) for author attribution.
                            </li>
                            <li>
                                I'm curious if these concepts can be obtained in an unsupervised fashion and later
                                interpreted (the assumption is that the concepts are obtained in a way where they are
                                (somehow) constrained to be more interpretable).
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Learning Interpretable Style Embeddings via Prompting LLMs (Patel et al., 2023)</summary>
                    <div class="content">

                        <p>
                            Creates dataset (StyleGenome). Two types of prompts (free-form and ones that target specific
                            features) are used to generate free-form natural language descriptions, which are then
                            formatted into multiple short-sentence descriptions to be used as styles (these are not
                            filtered, to maintain unsupervised setting).

                            To obtain LISA, a style-and-text to salience predictor (referred to as SFAM), which consists
                            of a T5 backbone with a linear binary classification layer trained with a contrastive
                            objective is used. This is further distilled into another model that simultaneously predicts
                            salience (between 0 and 1) for all (768) styles. Human-model agreement scores are seen to
                            increase with more training data. To obtain embeddings, \([0, 1]\) scores for each feature
                            are mapped with linear projection where the linear map is obtained by training distilled
                            SFAM with an additional linear layer using authorship attribution datasets.

                            A small fraction of the original generated styles are observed to refer to useless features.
                            Styles are also seen to be difficult to completely separate from content (which can be done
                            if we manually filter the styles). LISA also sometimes produces high scores for styles that
                            are relevant but opposite (likely moreso due to LISA's backbone's tendency).
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>
                                I wonder if creating a "hierarchy" of styles might help.
                            </li>
                            <li>
                                In the original Reddit scrape, I wonder if 10 documents even contain sufficient
                                information in pinning down who the author is.
                            </li>
                        </ul>
                        </p>
                    </div>
                </details>
                <details>
                    <summary>ICLEF: In-Context Learning with Expert Feedback for Explainable Style
                        Transfer (Saakyan and Muresan, 2024)</summary>
                    <div class="content">

                        <p> Studies attribute style transfer task (rewrite text as informal →
                            formal, etc.). No studies on explainability for textual style transfer.
                            Builds datasets for explainable style transfer using a proposed framework (ICLEF). Focuses
                            on formality and subjectivity style transfer and uses the GYAFC dataset (formality style
                            transfer dataset, source informal sentences collected from Yahoo Answers and target formal
                            variants collected through MTurk) and WNC dataset (Wikipedia-based dataset).
                            Collect improved versions of portions of original dataset using expert annotators. Verifies
                            semantic preservation quality using MIS (paraphrase quantification metric) and human
                            (expert) evaluation. Train models on collected explanations and text pairs.
                            Shows mild improvement on authorship attribution task using generated explanations for style
                            features.
                        </p>
                    </div>
                </details>
                <div class="paper-titles">Opinion/Perspective Summarization</div>
                <details>
                    <summary>P\(^3\)SUM: Preserving Author's Perspective in News Summarization
                        with
                        Diffusion Language Models (Liu et al., 2024)</summary>

                    <div class="content">

                        <p>
                            Motivation: Perspective-preserving summarization is understudied; LLMs might be politically
                            biased, and thus, summarization systems built on top of these LLMs will propagate bias. This
                            study evaluates the aptitude of LLMs to preserve political stances in summaries. It shows
                            LLMs struggle to preserve the author's perspective for summarization. Proposes P\(^3\)SUM, a
                            diffusion model-based political stance summarizer:
                        <ol>
                            <li>Train diffusion model on standard summarization data.</li>
                            <li>At each step:
                                <ol>
                                    <li>Generated output's stance is evaluated with a political stance classifier and
                                        compared to the target stance.</li>
                                    <li>Steers summary towards the target stance.</li>
                                </ol>
                            </li>
                        </ol>
                        P\(^3\)SUM doesn't require training on perspective summarization data due to the diffusion model
                        backbone. Its performance on ROUGE/abstractiveness suffers marginally while performing
                        significantly better on perspective summarization.
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Polarity Calibration for Opinion Summarization (Lei et al., 2024)</summary>
                    <div class="content">

                        <p>
                            Motivation: Authors observe that existing summarization systems amplify polarity bias (the
                            difference in representation between perspectives). Defines an intelligent summarizer as
                            “proportionally presenting both majority and minority opinions and aligning with the input
                            text polarity.” Develops a reward model focusing on three criteria:
                        <ul>
                            <li>
                                Whether the summary’s polarity distance matches the input text.
                            </li>
                            <li>
                                Whether the summary’s semantic content is faithful (using RoBERTa embeddings).
                            </li>
                            <li>
                                Whether the summary is coherent (using CoLA).
                            </li>
                        </ul>
                        Tested on two domains using automated/human evaluation.
                        </p>
                    </div>
                </details>
                <details>
                    <summary>NeuS: Neutral Multi-News Summarization for Mitigating Framing Bias (Lee et al., 2022)
                    </summary>
                    <div class="content">

                        <p>
                            Motivation: Studies framing bias (when journalists selectively choose content to cover) and
                            explores generating neutral summaries. Traditional framing-bias-free roundups are costly and
                            time-consuming. This study examines multi-document summarization for framing bias
                            mitigation. A new dataset from Allsides.com is used, along with a polarity-based metric for
                            framing bias aligned with human perceptions. The study also reveals that article titles
                            indicate framing bias, and hallucinated generations tend to have high framing bias.
                        </p>
                    </div>
                </details>
                <details>
                    <summary>Aspect-Controllable Opinion Summarization (Amplayo et al., 2021)</summary>
                    <div class="content">

                        <p> Examines a variant of summarization to obtain pinpointed summaries
                            for specific entities, introducing aspect controllers for focused summarization.
                            The model might not be directly applicable to our setting due to the need for custom
                            training data.</p>
                    </div>
                </details>
                <details>
                    <summary>Fair Abstractive Summarization of Diverse Perspectives (Zhang et al., 2023)</summary>
                    <div class="content">

                        <p> Summarization for text labeled with social values (e.g., male,
                            female). Defines fairness by matching social value ratios between source and generated
                            summaries.
                        </p>
                    </div>
                </details>
                <div class="paper-titles">Summary Evaluation</div>
                <details>
                    <summary>Automatically Evaluating Content Selection in Summarization without
                        Human Models (Louis et al., 2009)</summary>
                    <div class="content">

                        <p> Shows that distributional dissimilarity between source and
                            generated summaries correlates with manual PYRAMID metrics. Tested on TAC dataset; stemming
                            improves distributional correlation.</p>
                    </div>
                </details>
                <details>
                    <summary>Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation (Shapira et al., 2019)
                    </summary>
                    <div class="content">

                        <p> Original Pyramid involves SCU extraction and merging for summary
                            evaluation. Lightweight Pyramid uses fewer annotators without cross-referencing SCUs across
                            different references.
                            Can LLMs perform similar extraction when prompted?</p>
                        <p><strong>Thoughts:</strong> Raises the question of whether LLMs could perform similar
                            extraction tasks.</p>
                    </div>
                </details>
                <details>
                    <summary>Automatic Pyramid Evaluation Exploiting EDU-based Extractive
                        Reference Summaries (Hirao et al., 2018)</summary>
                    <div class="content">

                        <p> Automates PYRAMID by dividing source and reference summaries into
                            EDUs, identifying matches, and scoring based on EDU counts. Using ILP for this may be
                            overkill.</p>
                        <p><strong>Thoughts:</strong> The approach might be overcomplicated due to the use of ILP.</p>
                    </div>
                </details>
                <details>
                    <summary>Supervised Learning of Automatic Pyramid for Optimization-Based
                        Multi-Document Summarization (Peyrard et al., 2017)</summary>
                    <div class="content">

                        <p> Develops extractive summarization method relying on an auxiliary
                            function to approximate PYRAMID scores, using a genetic algorithm to select sentences.
                            Assumes a linear relationship between features of generated summaries and PYRAMID scores?
                        </p>
                        <p><strong>Thoughts:</strong> Assumes a linear relationship between summary features and Pyramid
                            scores.</p>
                    </div>
                </details>
                <details>
                    <summary>Automated Pyramid Summarization Evaluation (Gao et al., 2019)</summary>
                    <div class="content">

                        <p> Automates PYRAMID by segmenting and vectorizing summaries, using
                            EDUA for grouping and cosine similarity for evaluation.EDUA could be useful if we can gather
                            evidence lists.</p>
                        <p><strong>Thoughts:</strong> EDUA could be very useful if evidence lists can be reliably
                            gathered.</p>
                    </div>
                </details>
                <div class="paper-titles">Long-Context Summarization</div>
                <details>
                    <summary>Summary of a Haystack: A Challenge to Long-Context LLMs and RAG
                        Systems (Laban et al., 2024)</summary>
                    <div class="content">

                        <p> This work studies the Needle-in-a-Haystack summarization task and
                            benchmarks RAG versus long-context LLMs, measuring coverage annotated by humans.</p>
                    </div>
                </details>
                <details>
                    <summary>Open Domain Multi-document Summarization: A Comprehensive Study of
                        Model Brittleness under Retrieval (Giorgi et al., 2022)</summary>
                    <div class="content">

                        <p> Investigates the open-domain multi-document summarization setting,
                            focusing on extracting relevant information from large document sets to answer queries.</p>
                    </div>
                </details>
                <details>
                    <summary>Embrace Divergence for Richer Insights: A Multi-document
                        Summarization Benchmark and a Case Study on Summarizing Diverse Information from News
                        Articles (Huang et al., 2023)</summary>
                    <div class="content">

                        <p> Developed dataset DiverseSumm, with coverage measured through human
                            annotations, showing that LLMs generally struggle with coverage.</p>
                    </div>
                </details>
                <div class="paper-titles">Input Biases in LLMs</div>
                <details>
                    <summary>Understanding Position Bias Effects on Fairness in Social
                        Multi-Document Summarization (Olabisi et al., 2024)</summary>
                    <div class="content">

                        <p>
                            Investigates position bias in multi-document summarization for text from multiple dialects
                            in social settings. Most
                            position bias studies do not assess fairness. Uses DivSumm with three dialects in source
                            text, considering randomly shuffled versus ordered documents. Shows that:
                        <ul>
                            <li>
                                Reference summaries lack word overlap bias.
                            </li>
                            <li>
                                Abstractive models show no word overlap bias when documents are shuffled but favor
                                earlier parts when ordered.
                            </li>
                        </ul>
                        Position bias is conditional: models favor early text only when linguistically similar. Semantic
                        similarity between shuffled/ordered cases also measured; when ordered, summaries are biased
                        toward the first dialect group.
                        </p>
                        <p><strong>Thoughts:</strong> Unclear how semantic similarity is measured (mentions only cosine
                            similarity).</p>
                    </div>
                </details>
                <div class="paper-titles">Representation Degeneration Problem</div>
                <details>
                    <summary>Representation Degeneration Problem in Training Natural Language
                        Generation Models (Zhao et al., 2023)</summary>
                    <div class="content">

                        <p>
                        <ul>
                            <li>Studies geometry of embedding matrix for weight-tied transformer model</li>
                            <li>The embeddings of words that do not occur will minimize loss when norm is very large
                                (tends to infinity)</li>
                            <li>(Not strictly pointed out) Embeddings are dominated by first few singular components
                            </li>
                            <li>Adding simple regularization term to minimize average cosine similarity mitigates RDP
                            </li>
                        </ul>
                        </p>
                        <p><strong>Thoughts:</strong>
                        <ul>
                            <li>The paper indicates push-pull on Zipfian data causes anisotropy. Why not test this out
                                on (synthetic) uniformly generated sequence?</li>
                            <li>How did the anisotropy change? Provide some numbers? (even sth as simple as
                                average cosine similarity, not just SVD plots)</li>
                            <li>Why is RDP even an issue---if insufficient signals for softmax is the concern, why
                                can we not just scale all values by some large constant? They do show performance
                                increase by adding the regularization term, but it isn't clear if mitigating RDP
                                directly led to the performance increase.</li>
                            <li> What was the point of comparing geometry with NNs and Word2vec if
                                they aren't even going to compare downstream task performance? These (more or less)
                                satisfy isotropy better, but do they perform better or worse than transformers?</li>
                        </ul>
                        </p>
                    </div>
                </details>
            </div>
        </div>
    </div>
    <div id="footer-placeholder"></div>
    <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        $(function () {
            $("#navbar-placeholder").load("layouts/navbar.html");
            $("#footer-placeholder").load("layouts/footer.html");
        });
    </script>
</body>

</html>